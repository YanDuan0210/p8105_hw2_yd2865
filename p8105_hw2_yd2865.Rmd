---
title: "p8105_hw2_yd2865"
author: "Yan Duan"
date: "2025-09-16"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(readxl)
```

I'm an R Markdown document! 


## Problem 1

#### Clean the data in `pols-month.csv`
```{r}
pols_df = 
  read_csv("./fivethirtyeight_datasets/pols-month.csv", na = c("NA", ".", "")) |> 
  janitor::clean_names() |> 
  separate(mon, into = c("year", "month", "day"), sep = "-" ) |>   # Use `separate()` to break up the variable `mon`
  mutate(
    year = as.integer(year),
    month = factor(month.name[as.integer(month)],
         levels = month.name),
         president =         # Create a `president` variable
           case_when(
             prez_gop == 1 ~ "gop",
             prez_dem == 1 ~ "dem")) |> 
  select(year, month, president, everything(), -day, -prez_gop, -prez_dem) |>  # Remove three variables
  arrange(year, month)
```


#### Clean the data in `snp.csv` 
```{r}
snp_df = 
  read_csv("./fivethirtyeight_datasets/snp.csv", na = c("NA", ".", "")) |> 
  janitor::clean_names() |>
  mutate(
   date  = mdy(date),
   date = if_else(date > Sys.Date(), date %m-% years(100), date)) |> 
  separate(date, into = c("year", "month", "day"), sep = "-") |>
  mutate(
   year = as.integer(year),
   month = factor(month.name[as.integer(month)],
         levels = month.name)) |> 
  select(year, month, close) |> 
  arrange(year, month)
```


#### Tidy the `unemployment` data 

```{r}
unemp_df =
  read_csv("./fivethirtyeight_datasets/unemployment.csv") |>
  janitor::clean_names() |>
  pivot_longer(                # Switching from “wide” to “long” format
    jan:dec,
    names_to = "month",
    values_to = "unemp_rate") |>
   mutate(         # Make sure that key variables have the same name and same values
    year = as.integer(year),
    month = match(str_to_title(month), month.abb),
    month = factor(month.name[as.integer(month)],
         levels = month.name)) |> 
  arrange(year, month)
```

#### Joining datasets

```{r}
# Join the datasets by merging `snp` into `pols`

pols_snp =
  left_join(pols_df, snp_df, by = c("year", "month")) 

# Merging `unemployment` into the result
merged_df = 
  left_join(pols_snp, unemp_df, by = c("year", "month")) 
merged_df
```

This analysis utilized three datasets: `pols-month.csv`, `snp.csv` and `unemployment.csv`. 

* The `pols_df` dataset has 822 rows × 9 columns , after being organized, contains monthly data on the political structure of the United States starting from 1947, including `year`, `month`, `president` (the political party of the president, `gop` or `dem`), and the distribution numbers of state governors, senators, and representatives between the two parties (such as `gov_gop`, `sen_gop`, `rep_dem`). 
* The `snp_df` with 787 rows × 2 columns provides the monthly closing prices of the S&P 500 index during the same period (use variable `close`), with `year` and `month` serving as the keys. 
* The variable `unemp_df` with 68 rows × 13 columns represents the monthly unemployment rate in the United States. Originally, it was stored in columns for 12 months. After being organized, it was transformed into "long format" and the same keys of `year` and `month` were used.

During the cleaning process, the date variables in all three datasets were unified into `year` and `month`, and the months were standardized to the full English names of the months to ensure a smooth merge. Eventually, we used the `left_join()` function to first merge the `snp_df` into the `pols_df`, and then merge the `unemp_df`, resulting in the `merged_df` data frame which contains approximately 822 monthly observation records. The time span ranges from January 1947 to around June 2015. 

The final table integrates the political structure, the closing prices of the stock market, and the unemployment rate, providing a complete time series of data for analyzing the relationship between the political and economic fluctuations in the United States.


## Problem 2

#### Read "Mr. Trash Wheel" table
```{r}
mr_df =
  read_excel(
    path  = "./Trash Wheel/202509 Trash Wheel Collection Data.xlsx",
    sheet = "Mr. Trash Wheel",
    na    = c("NA", ".", ""),
    skip  = 1,
    range = cell_cols("A:N")) |>
  mutate(wheel = "Mr",
         Year = as.integer(Year))               
```

#### Read "Professor Trash Wheel" table

```{r}
prof_df =
  read_excel(
    path  = "./Trash Wheel/202509 Trash Wheel Collection Data.xlsx",
    sheet = "Professor Trash Wheel",
    na    = c("NA", ".", ""),
    skip  = 1,
    range = cell_cols("A:M")) |>
  mutate(wheel = "Professor")    
```

#### Read "Gwynns Falls Trash Wheel" table

```{r}
gwyn_df =
  read_excel(
    path  = "./Trash Wheel/202509 Trash Wheel Collection Data.xlsx",
    sheet = "Gwynns Falls Trash Wheel",
    na    = c("NA", ".", ""),
    skip  = 1,
    range = cell_cols("A:L")) |>
  mutate(wheel = "Gwynnda")    
```

#### Tidy and clean data

```{r}
wheel_tidy = 
  bind_rows(mr_df, prof_df, gwyn_df) |>   # Merge the three datasets
  janitor::clean_names() |> 
  rename(                      # Change the variable name
      dumpster = dumpster,
      month = month,
      year  = year,
      date  = date,
      weight = starts_with("weight"),
      volume_cubic_yards = starts_with("volume"),
      plastic_bottles = starts_with("plastic_bottles"),
      polystyrene = starts_with("polystyrene"),
      cigarette = starts_with("cigarette"),
      glass = starts_with("glass"),
      plastic = starts_with("plastic"),
      wrappers = starts_with("wrappers"),
      sports = starts_with("sports"),
      homes = starts_with("homes")) |> 
  filter(!is.na(dumpster)) |>     # Remove the rows that do not have `dumpster` data
  mutate(date = as.Date(date),
         sports = ifelse(is.na(sports), NA_integer_, as.integer(round(sports))))# Round the number of sports balls to the nearest integer and convert the result into an integer variable.
wheel_tidy
```

#### The total number of observations after merging the datasets

```{r}
n_obs = nrow(wheel_tidy)
```

#### Calculate the total weight of the trash of "Professor Trash Wheel"
```{r}
prof_total_weight =
  wheel_tidy |>
  drop_na(weight) |>
  filter(wheel == "Professor") |> 
  summarise(total_weight = format(round(sum(weight), 2), nsmall = 2))
```

#### Calculate the total number of cigarette butts collected by Gwynnda in June 2022
```{r}
gwyn_cigs_jun22 =
  wheel_tidy |>
  drop_na(weight) |> 
  filter(wheel == "Gwynnda",
         year(date) == 2022, month(date) == 6) |>
  summarise(total_cigarette = sum(cigarette))
```

This analysis utilized three datasets: "Mr. Trash Wheel", "Professor Trash Wheel" and "Gwynnda Falls Trash Wheel".

* `mr_df` represents the trash collection records of "Mr. Trash Wheel", including `dumpster` (trash bin number), `month`, `year`, `date`, `weight` (tons), `volume_cubic_yards` (cubic yards), and various variables representing the quantities of different types of trash items (such as `plastic_bottles`, `polystyrene`, `cigarette`, `glass`, `wrappers`, `sports`, `homes`).
* `prof_df` and `gwyn_df` contains similar records for "Professor Trash Wheel" and "Gwynnda Falls Trash Wheel", with the same variable structure as `mr_df`.

We used the `bind_rows()` function to combine the three datasets into a tidy data frame named `wheel_tidy`.
During the cleaning process, we standardized the variable names and data formats for the three datasets, and rounded the value of `sports` to an integer.
To distinguish the data from different devices, we added a variable `wheel` for each sub-dataset, with the values being `Mr`, `Professor` and `Gwynnda` respectively.

The post-organized `wheel_tidy` dataset contains a total of **`r n_obs`** observation records. Using the functions `filter()` and `summarise()` to conduct conditional filtering and group summarization on the data, the total weight of Professor's trash collection is **`r prof_total_weight`** tons and Gwynnda collected **`r format(gwyn_cigs_jun22$total_cigarette, big.mark = ",")`** cigarette butts in June 2022, which comprehensively reflects the trash collection situation of the three devices since their respective commissioning.


## Problem 3

#### Read, tidy and clean the data in `Zip Codes.csv` 


```{r}
zip_df = 
  read_csv("./zillow_data/Zip Codes.csv", na = c("NA", ".", "")) |> 
  janitor::clean_names() |> 
  mutate(
    zipcode = as.integer(zip_code),
    neighborhood = as.character(neighborhood),
    country = as.character(county),
    borough = recode(county,     # Recoded into the 5 borough of New York City
                     "Bronx"   = "Bronx",
                     "Kings"   = "Brooklyn",
                     "New York"= "Manhattan",
                     "Queens"  = "Queens",
                     "Richmond"= "Staten Island")) |>
  select(county, zipcode, neighborhood, borough) 
```

#### Read, tidy and clean the Zillow data

```{r}
zori_df = 
  read_csv("./zillow_data/Zip_zori_uc_sfrcondomfr_sm_month_NYC.csv", na = c("NA", ".", "")) |> 
  janitor::clean_names() |> 
  rename(zipcode = region_name) |> 
  mutate(
    county_name = str_replace(county_name, "county", ""),
    zipcode = as.integer(zipcode)) |>
  pivot_longer(       # Arrange the date data into a single column
    cols = 10:125,              
    names_to  = "date",
    values_to = "zori",
    names_prefix = "x") |> 
  select(region_id, zipcode, county_name, date, zori) |> 
  arrange(zipcode, date)
```

#### Joining datasets
```{r}
zip_info_one =     # Remove duplicate data
  zip_df |>
  mutate(zipcode = as.integer(zipcode)) |>
  distinct(zipcode, .keep_all = TRUE)
zori_tidy = 
  zori_df |>
  left_join(zip_info_one, by = "zipcode") |>
  arrange(zipcode, date) |> 
  select(zipcode, borough, neighborhood, date, zori,
         county = county_name) |>
  arrange(zipcode, date, borough,neighborhood)
zori_tidy
```

#### Problem 3.1

```{r}
summary_df =
  zori_tidy |>
  summarise(
    n_obs = nrow(zori_tidy),       # The number of observations                     
    unique_zipcode = length(unique(zipcode)),  # The number of unique ZIP codes
    unique_hood = length(unique(neighborhood)))  # The number of unique neighborhoods
summary_df
```

The "ZIP Codes" data and "Zillow" data were integrated into a neat dataset named `zori_tidy`. The generated result shows that there are a total of 17,284 rental observation data, covering 149 ZIP codes and distributed across 43 neighborhoods.


#### Problem 3.2

```{r}
zip_in_zipcode =
  zip_df |>
  anti_join(unique(zori_df["zipcode"]), by = "zipcode") |>
  arrange(borough, zipcode)
zip_in_zipcode
```

The reason why the postal codes might be excluded from the Zillow dataset is:

* Some data quality filtering: Abnormal noise or unstable sequences are excluded.
* Small areas at the periphery such as airports or cross-county boundaries may not have been collected. 
* Transaction volume is too low, and Zillow does not publish it.


#### Problem 3.3

```{r}
zori_2020 = 
  zori_tidy |>
  filter(date == "2020_01_31") |> 
  select(zipcode, borough, neighborhood, Jan2020 = zori)
zori_2021 =
  zori_tidy |>
  filter(date == "2021_01_31") |> 
  select(zipcode, borough, neighborhood, Jan2021 = zori)
zori_change = 
  zori_tidy |>
  distinct(zipcode, borough, neighborhood) |> 
  left_join(zori_2020, by = "zipcode") |> 
  left_join(zori_2021, by = "zipcode") |> 
  mutate(change = Jan2021 - Jan2020) |> 
  arrange(change)
drop_top10 = 
  zori_change |> 
  slice_head(n = 10) |> 
  select(
    zipcode, borough, neighborhood,
    price_2020_01 = Jan2020,
    price_2021_01 = Jan2021,
    change) |> 
  mutate(
    price_2020_01 = format(round(price_2020_01, 2), nsmall = 2),
    price_2021_01 = format(round(price_2021_01, 2), nsmall = 2),
    change       = format(round(change, 2), nsmall = 2))
```

```{r}
knitr::kable(drop_top10,
             col.names = c("zipcode", "borough", "neighborhood",
                           "price_2020_01", "price_2021_01", "Change"))
```

Based on the rental data from `2020_01_31` and `2021_01_31` in the "Zillow" dataset, identify the ZIP Code in New York City where the rental rate decreased the most during the COVID-19 pandemic. This data covers the five boroughs of New York and their neighborhoods, and it can reflect the significant fluctuations in housing rents before and after the epidemic.

According to the generated data, the top 10 ZIP Codes with the most significant rent decreases are all located in Manhattan, with the rent drops ranging from -684.93 to -912.60 dollars. It indicates that the impact of the epidemic was most significant on the rents in the core residential areas of Manhattan.